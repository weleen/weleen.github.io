<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="weleen's blog"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.4"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.4"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>A Survey of Appearance Models in Visual Object Tracking | Welcome to weleen's blog</title><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">A Survey of Appearance Models in Visual Object Tracking</h1><a id="logo" href="/.">Welcome to weleen's blog</a><p class="description">Pursue myself</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="search"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">A Survey of Appearance Models in Visual Object Tracking</h1><div class="post-meta"><a href="/2016/05/31/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/#comments" class="comment-count"><i data-disqus-identifier="2016/05/31/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/" class="disqus-comment-count"></i>Guestbook</a><p><span class="date">May 31, 2016</span><span><a href="/categories/Computer-Science/" class="category">Computer Science</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>Hits</i></i></span></p></div><div class="post-content"><p>最近看的一篇综述论文 <a href="http://arxiv.org/abs/1303.4803" target="_blank" rel="noopener">http://arxiv.org/abs/1303.4803</a></p>
<a id="more"></a>

<p>What’s difficult in this research?<br>研究的难点？<br>Handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion.<br>目标可能由于旋转、遮挡、形变的原因导致外形变化。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="appearance-modeling"><a href="#appearance-modeling" class="headerlink" title="appearance modeling"></a>appearance modeling</h3><p>The most important issue is effective modeling of the 2D appearance of tarcked objects.<br>主要需要解决的是对2维目标的外观建模，分解成了视觉表达和统计建模。<br>The appearance modeling is decomposed into this two stage:</p>
<ul>
<li>visual representation</li>
<li>statistical modeling</li>
</ul>
<h3 id="overview-of-object-tracking"><a href="#overview-of-object-tracking" class="headerlink" title="overview of object tracking"></a>overview of object tracking</h3><p>The typical object tracking system is composed of four modules</p>
<ol>
<li>Object Initialization. manual or automatic, manual is performed by user to annotate the object, automatic is achieved by object detectors.</li>
<li>Appearance Modeling. It consists of two components: visual representation and statistical modeling.</li>
<li>Motion Estimation. This is formulated as a dynamic state estimation problem:<br>$x_t = f(x_{t-1},v_{t-1})$ and $z_t = h(x_t,w_t)$<br>where $x_t$ is the current state, $f$ is the state evolution function, $v_{t−1}$ is the evolution process noise, $z_t$ is the current observation, $h$ denotes the measurement function, and $w_t$ is the measurement noise.</li>
<li>Object Localization.</li>
</ol>
<p>整个目标跟踪系统被分成：1、目标初始化；2、外观建模；3、运动预测；4、目标定位。</p>
<p>The Orgnization of the survey likes the figure under.<br><img src="/img/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/1.png" alt="orgnization of the survey"></p>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ol>
<li>Review the literature of visual representations from a feature-construction viewpoint. Categorize the representation into local and global features.</li>
<li>Classify the existing model into generative, discriminative, and hybrid generative-discriminative.</li>
<li>Give some benchmark resources.</li>
</ol>
<p>贡献如上</p>
<h2 id="Visual-Represetation"><a href="#Visual-Represetation" class="headerlink" title="Visual Represetation"></a>Visual Represetation</h2><p>Representation is classified into local and global to introduce.<br>视觉表达可以分成全局和局部。</p>
<h3 id="Global"><a href="#Global" class="headerlink" title="Global"></a>Global</h3><h4 id="Raw-Pixel-Representation"><a href="#Raw-Pixel-Representation" class="headerlink" title="Raw Pixel Representation"></a>Raw Pixel Representation</h4><p>Simple and efficient, construct in vector-based or matrix-based.</p>
<h4 id="Optical-Flow-Representation"><a href="#Optical-Flow-Representation" class="headerlink" title="Optical Flow Representation"></a>Optical Flow Representation</h4><p>It has two branches: constant-brightness-constraint optical flow and non-brightness-constraint optical flow.</p>
<h4 id="Histogram-Representation"><a href="#Histogram-Representation" class="headerlink" title="Histogram Representation"></a>Histogram Representation</h4><h5 id="single-cue"><a href="#single-cue" class="headerlink" title="single-cue"></a>single-cue</h5><p>encode the information in the object region.</p>
<h5 id="multi-cue"><a href="#multi-cue" class="headerlink" title="multi-cue"></a>multi-cue</h5><p>encode more information to enhance the robusness of visual represetation.</p>
<p>a. Spatial Color<br>joint spatial-color modeling(describe the distribution properties of object appearance in a joint spatial-color space) and patch-division.</p>
<p>b. Spatial Texture<br>An estimate of the joint spatial-texture probability is made to capture the distribution information on object appearance.</p>
<p>c. Shape Texture</p>
<h4 id="Covariance-Representations"><a href="#Covariance-Representations" class="headerlink" title="Covariance Representations"></a>Covariance Representations</h4><p>the covariance matrix representations can be divided into two branches: affine-invariant Riemannian metric-based and log-Euclidean Riemannian metric-based.</p>
<ol>
<li>The affine-invariant Riemannian metric</li>
</ol>
<p>$$\rho(\textbf{C}<em>1,\textbf{C}_2)=\sqrt{\sum</em>{j=1}^d ln^2\lambda_j(\mathbf{C}_1,\textbf{C}_2)}$$</p>
<ol start="2">
<li>The log-Euclidean Riemannian metric</li>
</ol>
<p>$$d(C_i,C_j)=||\log (C_i,C_j)||$$</p>
<h4 id="Wavelet-Filtering-Based-Representation"><a href="#Wavelet-Filtering-Based-Representation" class="headerlink" title="Wavelet Filtering-Based Representation"></a>Wavelet Filtering-Based Representation</h4><p>a wavelet filtering-based representation takes advantage of wavelet transforms to filter the object region in different scales or directions</p>
<h4 id="Active-Contour-Representation"><a href="#Active-Contour-Representation" class="headerlink" title="Active Contour Representation"></a>Active Contour Representation</h4><p>In order to track the nonrigid objects, active contour representations have been widely used.<br><img src="/img/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/2.png" alt="active contour representation"></p>
<h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p>原始的pixel特征简单，但是只考虑了颜色信息，不能克服光照变化。</p>
<p>在亮度不变的前提下，CBC光流能包含场信息，但是假设是亮度不变，所以不适应有噪点、光照变化和局部形变，NBC光流由此改进。</p>
<p>single-cue直方图作为统计特征，会失去目标的空间特性，一般会收到背景的影响，spatial-color直方图可以结合空间特性，shape-texture直方图包含形状和纹理信息，可以克服光照变化和姿势变化。</p>
<p>互相关矩阵有以下优点：</p>
<ol>
<li>能使用目标表观的互相关性；</li>
<li>能够融合不同模态的图像特征；</li>
<li>维度低，计算高效；</li>
<li>能对比不同大小的区域；</li>
<li>比较容易实现；</li>
<li>光照不变、抗形变和遮挡。</li>
</ol>
<p>缺点有：</p>
<ol>
<li>容易受噪声影响；</li>
<li>丢失了很多信息，比如纹理、形状、位置。</li>
</ol>
<p>小波滤波通过小波变换结合了局部的纹理信息，多尺度多方向表达了目标的统计特性。</p>
<p>活动轮廓可以解决非刚性物体的跟踪。</p>
<h3 id="Local-Feature-Based-Visual-Representation"><a href="#Local-Feature-Based-Visual-Representation" class="headerlink" title="Local Feature-Based Visual Representation"></a>Local Feature-Based Visual Representation</h3><p>Classify into seven classes: local template-based, segmentation-based, SIFT-based, MSER-based, SURF-based, corner feature-based, feature pool-based, and saliency detection-based</p>
<h4 id="Local-Template-Based"><a href="#Local-Template-Based" class="headerlink" title="Local Template-Based"></a>Local Template-Based</h4><p>Represent an object region using a set of part templates.</p>
<h4 id="Segmentation-Based"><a href="#Segmentation-Based" class="headerlink" title="Segmentation-Based"></a>Segmentation-Based</h4><p>A segmentation-based visual representation incorporates the image segmentation cues</p>
<h4 id="SIFT-based"><a href="#SIFT-based" class="headerlink" title="SIFT-based"></a>SIFT-based</h4><p>Making use of the SIFT features inside an object region to describe the structural information of ob-ject appearance.</p>
<h4 id="MSER-Based"><a href="#MSER-Based" class="headerlink" title="MSER-Based"></a>MSER-Based</h4><p>An MSER-based visual representation needs to extract the MSER (maximally stable extremal region) features for visual representation</p>
<h4 id="SURF-Based"><a href="#SURF-Based" class="headerlink" title="SURF-Based"></a>SURF-Based</h4><p>the SURF (Speeded Up Robust Feature) is a variant of SIFT</p>
<h4 id="Corner-Feature-Based"><a href="#Corner-Feature-Based" class="headerlink" title="Corner Feature-Based"></a>Corner Feature-Based</h4><h4 id="Local-Feature-Pool-Based"><a href="#Local-Feature-Pool-Based" class="headerlink" title="Local Feature-Pool-Based"></a>Local Feature-Pool-Based</h4><h4 id="Saliency-Detection-Based"><a href="#Saliency-Detection-Based" class="headerlink" title="Saliency Detection-Based"></a>Saliency Detection-Based</h4><p>Saliency detection is inspired by the focus-of-attention (FoA) theory</p>
<h4 id="Discussion-1"><a href="#Discussion-1" class="headerlink" title="Discussion"></a>Discussion</h4><p>基于局部模板的表达，能够应对遮挡问题。<br>基于分割的表达可以得到目标的结构化特征（例如目标边缘和超像素）。<br>基于SIFT特征的表达对光照、形变和遮挡有鲁棒性，但是不能包含目标的大小，方向和姿势等特征。<br>基于MSER（最大稳定极值区域）的表达可以容忍噪音但是不能克服光照变化。<br>基于SURF特征的表达具有尺度不变、旋转不变和计算高效的优点。<br>基于角点的特征表达对有大量角点的目标比较适用，但是不能应对形变和噪声。<br>基于池化的特征表达。<br>基于Saliency的特征表达。</p>
<h2 id="Statistical-Modeling-For-Tracking-By-Detection"><a href="#Statistical-Modeling-For-Tracking-By-Detection" class="headerlink" title="Statistical Modeling For Tracking-By-Detection"></a>Statistical Modeling For Tracking-By-Detection</h2><p>The statistical modeling is classified into three categories, including generative, discriminative, and hybrid generatice-discriminative.</p>
<p>The generative appearance models mainly concentrate on how to accurately fit the data from the object class, but suffer from distractions caused by the background regions with similar appearance to the object class.</p>
<p>Discriminative appearance models pose visual object tracking as a<br>binary classification issue. They aim to maximize the separability between the object and non-object regions discriminately. A major limitation of the discriminative appearance models is to rely heavily on training sample selection.</p>
<p>Due to taking a heuristic fusion strategy, HGDAMs(<strong>hybrid generative-discriminative appearance models</strong>) cannot guarantee that the performance of the hybrid models after information fusion is better than those of the individual models.</p>
<h3 id="Mixture-Generative-Appearance-Models"><a href="#Mixture-Generative-Appearance-Models" class="headerlink" title="Mixture Generative Appearance Models"></a>Mixture Generative Appearance Models</h3><p>This type of generative appearance model adaptively learns several compo- nents for capturing the spatiotemporal diversity of object appearance containing WSL mixture models and Gaussian mixture models.</p>
<h4 id="WSL-Mixture-Models"><a href="#WSL-Mixture-Models" class="headerlink" title="WSL Mixture Models"></a>WSL Mixture Models</h4><p>the WSL mixture model contains the following three components: W-component, S-component, and L- component.<br><a href="http://www.cs.toronto.edu/~fleet/research/Papers/cvpr-01A.pdf" target="_blank" rel="noopener">Robust Online Appearance Models for Visual Tracking</a> and<br><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1344039" target="_blank" rel="noopener">Visual Tracking and Recognition Using Appearance-Adaptive Models in Particle Filters</a>.</p>
<h4 id="Gaussian-Mixture-Models"><a href="#Gaussian-Mixture-Models" class="headerlink" title="Gaussian Mixture Models"></a>Gaussian Mixture Models</h4><p>In essence, the Gaussian mixture models utilize a set of Gaussian distributions to approximate the underlying density function of object appearance.<br><img src="/img/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/3.png" alt="GMMs"></p>
<h3 id="Kernel-Based-Generative-Appearance-Models"><a href="#Kernel-Based-Generative-Appearance-Models" class="headerlink" title="Kernel-Based Generative Appearance Models"></a>Kernel-Based Generative Appearance Models</h3><p>Kernel-based generative appearance models(KGAMs)utilize kernel density estimation to construct kernel-based visual representations and then carry out the mean shift for object localization.<br><img src="/img/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/4.png" alt="KGAMs"></p>
<p>Including color-driven KGAMs, shape-integration KGAMs, scale-aware KGAMs, nonsymmetric KGAMs, KGAMs by global mode seeking, and sequential-kernel-learning KGAMs.</p>
<h3 id="Subspace-Learning-Based-Generative-Appearance-Models"><a href="#Subspace-Learning-Based-Generative-Appearance-Models" class="headerlink" title="Subspace Learning-Based Generative Appearance Models"></a>Subspace Learning-Based Generative Appearance Models</h3><p>According to the used techniques for subspace analysis, they can be categorized into two types: conventional and unconventional SLGAMs.</p>
<h3 id="Boosting-Based-Discriminative-Appearance-Models"><a href="#Boosting-Based-Discriminative-Appearance-Models" class="headerlink" title="Boosting-Based Discriminative Appearance Models"></a>Boosting-Based Discriminative Appearance Models</h3><p>According to the learning strategies employed, they can be categorized into self-learning and co-learning BDAMs.</p>
<p>Self-learning-based BDAMs adopt the self-learning strategy to learn object/non-object classifier.In order to tackle this problem, co-learning-based BDAMs are developed to capture the discriminative information from many unlabeled samples in each frame.</p>
<h3 id="SVM-Based-Discriminative-Appearance-Models"><a href="#SVM-Based-Discriminative-Appearance-Models" class="headerlink" title="SVM-Based Discriminative Appearance Models"></a>SVM-Based Discriminative Appearance Models</h3><p>SDAMs aim to learn margin-based discriminative SVM classifiers for maximizing in- terclass separability. SDAMs are typically based on self-learning SDAMs and co-learning SDAMs.</p>
<h3 id="Randomized-Learning-Based-Discriminative-Appearance-Models"><a href="#Randomized-Learning-Based-Discriminative-Appearance-Models" class="headerlink" title="Randomized Learning-Based Discriminative Appearance Models"></a>Randomized Learning-Based Discriminative Appearance Models</h3><p>In principle, randomized learning techniques can build a diverse classifier ensemble by performing random input selec- tion and random feature selection.</p>
<h3 id="Discriminant-Analysis-Based-Discriminative-Appearance-Models"><a href="#Discriminant-Analysis-Based-Discriminative-Appearance-Models" class="headerlink" title="Discriminant Analysis-Based Discriminative Appearance Models"></a>Discriminant Analysis-Based Discriminative Appearance Models</h3><p>In principle, its goal is to find a low-dimensional subspace with a high interclass separability. According to the learning schemes used, it can be split into two branches: conventional discriminant analysis and graph-driven discriminant analysis. However, a drawback is that these algorithms need to retain a large amount of labeled/unlabeled samples for graph learning, leading to their impracticality for real tracking applications.</p>
<h3 id="Codebook-Learning-Based-Discriminative-Appearance-Models"><a href="#Codebook-Learning-Based-Discriminative-Appearance-Models" class="headerlink" title="Codebook Learning-Based Discriminative Appearance Models"></a>Codebook Learning-Based Discriminative Appearance Models</h3><p>CLDAMs need to constructed the foreground and background codebooks to adaptively capture the dynamic appearance information from the foreground and background.</p>
<p>实际上codebook需要的训练集太大，在实际应用中不是很方便。</p>
<h3 id="Hybrid-Generative-Discriminative-Appearance-Models"><a href="#Hybrid-Generative-Discriminative-Appearance-Models" class="headerlink" title="Hybrid Generative-Discriminative Appearance Models"></a>Hybrid Generative-Discriminative Appearance Models</h3><p>the generative and the discriminative mod- els have their own advantages and disadvantages and are complementary to each other to some extent.<br>结合产生和判别模型的优点。</p>
</div><div class="post-copyright"><blockquote><p>Original author: weleen</p><p>Original link: <a href="http://weleen.github.io/2016/05/31/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/">http://weleen.github.io/2016/05/31/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/</a></p><p>Copyright Notice: Please indicate the source of the reprint (must retain the author's signature and link)</p></blockquote></div><div class="tags"><a href="/tags/Object-Tracking/">Object Tracking</a></div><div class="post-share"><div class="social-share"><span>Share:</span></div></div><div class="post-nav"><a href="/2016/06/05/Object-Tracking-A-Survey/" class="pre">Object Tracking: A Survey</a></div><div id="comments"><div id="disqus_thread"></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">Contents</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#appearance-modeling"><span class="toc-text">appearance modeling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#overview-of-object-tracking"><span class="toc-text">overview of object tracking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contribution"><span class="toc-text">Contribution</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Visual-Represetation"><span class="toc-text">Visual Represetation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Global"><span class="toc-text">Global</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Raw-Pixel-Representation"><span class="toc-text">Raw Pixel Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Optical-Flow-Representation"><span class="toc-text">Optical Flow Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Histogram-Representation"><span class="toc-text">Histogram Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#single-cue"><span class="toc-text">single-cue</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#multi-cue"><span class="toc-text">multi-cue</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Covariance-Representations"><span class="toc-text">Covariance Representations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Wavelet-Filtering-Based-Representation"><span class="toc-text">Wavelet Filtering-Based Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Active-Contour-Representation"><span class="toc-text">Active Contour Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Discussion"><span class="toc-text">Discussion</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Local-Feature-Based-Visual-Representation"><span class="toc-text">Local Feature-Based Visual Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Local-Template-Based"><span class="toc-text">Local Template-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Segmentation-Based"><span class="toc-text">Segmentation-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SIFT-based"><span class="toc-text">SIFT-based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MSER-Based"><span class="toc-text">MSER-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SURF-Based"><span class="toc-text">SURF-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Corner-Feature-Based"><span class="toc-text">Corner Feature-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Local-Feature-Pool-Based"><span class="toc-text">Local Feature-Pool-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Saliency-Detection-Based"><span class="toc-text">Saliency Detection-Based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Discussion-1"><span class="toc-text">Discussion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Statistical-Modeling-For-Tracking-By-Detection"><span class="toc-text">Statistical Modeling For Tracking-By-Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mixture-Generative-Appearance-Models"><span class="toc-text">Mixture Generative Appearance Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#WSL-Mixture-Models"><span class="toc-text">WSL Mixture Models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gaussian-Mixture-Models"><span class="toc-text">Gaussian Mixture Models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernel-Based-Generative-Appearance-Models"><span class="toc-text">Kernel-Based Generative Appearance Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Subspace-Learning-Based-Generative-Appearance-Models"><span class="toc-text">Subspace Learning-Based Generative Appearance Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Boosting-Based-Discriminative-Appearance-Models"><span class="toc-text">Boosting-Based Discriminative Appearance Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-Based-Discriminative-Appearance-Models"><span class="toc-text">SVM-Based Discriminative Appearance Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Randomized-Learning-Based-Discriminative-Appearance-Models"><span class="toc-text">Randomized Learning-Based Discriminative Appearance Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Discriminant-Analysis-Based-Discriminative-Appearance-Models"><span class="toc-text">Discriminant Analysis-Based Discriminative Appearance Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Codebook-Learning-Based-Discriminative-Appearance-Models"><span class="toc-text">Codebook Learning-Based Discriminative Appearance Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hybrid-Generative-Discriminative-Appearance-Models"><span class="toc-text">Hybrid Generative-Discriminative Appearance Models</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/11/26/Bit-Scalable-Deep-Hashing-With-Regularized-Similarity-Learning-for-Image-Retrieval-and-Person-Re-Identification/">Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/23/Supervised-Hashing-for-Image-Retrieval-via-Image-Representation-Learning/">Supervised Hashing for Image Retrieval via Image Representation Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/23/Simultaneous-Feature-Learning-and-Hash-Coding-with-Deep-Neural-Networks/">Simultaneous Feature Learning and Hash Coding with Deep Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/21/awesome-hash/">awesome-hash</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/09/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%90%86%E8%A7%A3/">最大熵模型的理解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/21/Unsupervised-Cross-dataset-Person-Re-identification-by-Transfer-Learning-of-Spatial-Temporal-Pattern/">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Pattern</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/12/Self-similarity-Grouping-A-Simple-Unsupervised-Cross-Domain-Adaptation-Approach-for-Person-Re-identification/">Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/10/STA-Spatial-Temporal-Attention-for-Large-Scale-Video-based-Person-Re-Identification/">STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/10/Cross-view-Asymmetric-Metric-Learning-for-Unsupervised-Person-Re-identification/">Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/24/awesome-reid/">awesome-reid</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Computer-Science/">Computer Science</a><span class="category-list-count">17</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Computer-Science/Collections/">Collections</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> Tags</i></div><div class="tagcloud"><a href="/tags/ReID/" style="font-size: 15px;">ReID</a> <a href="/tags/Unsupervised/" style="font-size: 15px;">Unsupervised</a> <a href="/tags/Cross-view/" style="font-size: 15px;">Cross-view</a> <a href="/tags/Metric-Learning/" style="font-size: 15px;">Metric Learning</a> <a href="/tags/Object-Tracking/" style="font-size: 15px;">Object Tracking</a> <a href="/tags/Deep-Hash/" style="font-size: 15px;"> Deep Hash</a> <a href="/tags/DNN/" style="font-size: 15px;">DNN</a> <a href="/tags/Hard-Negtive-Mining/" style="font-size: 15px;">Hard Negtive Mining</a> <a href="/tags/Hand-Pose-Estimation/" style="font-size: 15px;">Hand Pose Estimation</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Video-Person-ReID/" style="font-size: 15px;">Video Person ReID</a> <a href="/tags/Attention/" style="font-size: 15px;">Attention</a> <a href="/tags/Deep-Hash/" style="font-size: 15px;">Deep Hash</a> <a href="/tags/Grouping/" style="font-size: 15px;">Grouping</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/Unsupervised-ReID/" style="font-size: 15px;">Unsupervised ReID</a> <a href="/tags/Transfer-Learning/" style="font-size: 15px;">Transfer Learning</a> <a href="/tags/hash/" style="font-size: 15px;">hash</a> <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" style="font-size: 15px;">概率论</a> <a href="/tags/%E6%9C%80%E5%A4%A7%E7%86%B5/" style="font-size: 15px;">最大熵</a> <a href="/tags/%E7%86%B5/" style="font-size: 15px;">熵</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> Archive</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/">2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/">2016</a><span class="archive-list-count">7</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Site Map</a> |  <a href="/atom.xml">Subscribe to this site</a> |  <a href="/about/">Contact the blogger</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">weleen.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/" target="_blank" rel="noopener"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-101752082-1','auto');ga('send','pageview');
</script><script type="text/javascript" src="/js/search.json.js?v=2.0.4"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML" async></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.4" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.4" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script><script>var disqus_shortname = 'https-weleen-github-io';
var disqus_identifier = '2016/05/31/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/';
var disqus_title = 'A Survey of Appearance Models in Visual Object Tracking';
var disqus_url = 'http://weleen.github.io/2016/05/31/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//https-weleen-github-io.disqus.com/count.js" async></script><script type="text/javascript" src="//https-weleen-github-io.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></body></html>