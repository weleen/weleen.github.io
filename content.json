[{"title":"Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification","date":"2019-11-25T16:44:01.000Z","path":"2019/11/26/Bit-Scalable-Deep-Hashing-With-Regularized-Similarity-Learning-for-Image-Retrieval-and-Person-Re-Identification/","text":"TIP2015 paper，除了利用deep learning，为了能够实现变长的hash code，学习了weight matrix来对hamming distance做加权。","tags":[{"name":" Deep Hash","slug":"Deep-Hash","permalink":"http://weleen.github.io/tags/Deep-Hash/"}]},{"title":"Supervised Hashing for Image Retrieval via Image Representation Learning","date":"2019-11-22T21:16:04.000Z","path":"2019/11/23/Supervised-Hashing-for-Image-Retrieval-via-Image-Representation-Learning/","text":"AAAI2014论文，第一篇基于Deep Learning的Learning to hash文章，采用的双阶段方法，先把由ground truth得到的similarity matrix进行decomposition，得到近似的target hash code，再学习image到target hash code的映射。","tags":[{"name":"Deep Hash","slug":"Deep-Hash","permalink":"http://weleen.github.io/tags/Deep-Hash/"}]},{"title":"Simultaneous Feature Learning and Hash Coding with Deep Neural Networks","date":"2019-11-22T20:59:38.000Z","path":"2019/11/23/Simultaneous-Feature-Learning-and-Hash-Coding-with-Deep-Neural-Networks/","text":"CVPR2015 deep hash论文，采用triplet loss并提出一个divide-and-encode module。 ![](\\img\\Simultaneous-Feature-Learning-and-Hash-Coding-with-Deep-Neural-Networks\\note.jpg]","tags":[{"name":"Deep Hash","slug":"Deep-Hash","permalink":"http://weleen.github.io/tags/Deep-Hash/"}]},{"title":"awesome-hash","date":"2019-11-20T21:21:10.000Z","path":"2019/11/21/awesome-hash/","text":"Collections 1. Statistics 2. Survey 3. Unsupervised Hash (+ Transfer / Semi-supervised learning) 4. Supervised Hash 5. Others 6. Datasets 1. Statistics Conference Link #Total Unsupervised Hash Supervised Hash Others Datasets ICCV2019 Click - - - - - CVPR2019 Click - - - - - ECCV2018 Click - - - - - CVPR2018 Click - - - - - ICCV2017 Click - - - - - CVPR2017 Click - - - - - 2. Survey 1) *”A Survey of Open-World Person Re-identification”*, TCSVT 2019, [paper] 3. Unsupervised Hash4. Supervised Hash 1) *”Supervised Hashing for Image Retrieval via Image Representation Learning”*, AAAI 2014, [paper]]code] 2) *”Simultaneous Feature Learning and Hash Coding with Deep Neural Networks”*, CVPR2015, [paper] 3) *”Simultaneous Feature Learning and Hash Coding with Deep Neural Networks”*, TIP2015, [paper][code] 5. Others6. Datasets 1) CIFAR10 2) MNIST 3) LABELME 4) NUS-WIDE 5) ImageNet","tags":[{"name":"hash","slug":"hash","permalink":"http://weleen.github.io/tags/hash/"}]},{"title":"最大熵模型的理解","date":"2019-11-08T20:43:24.000Z","path":"2019/11/09/最大熵模型的理解/","text":"最近在看一些无监督聚类的文章，发现了很有意思的博客，里面讲了很多关于数学、物理、天文还有信息科学的内容。在看完博主关于最大熵的内容后也写下一点心得。 首先是熵的定义，离散概率和连续概率形式：$$S(x)=-\\sum_{x} p(x) \\log p(x)\\tag{1}$$$$S(x)=-\\int p(x)\\log p(x) dx\\tag{2}$$ 其实之间看到这个定义是比较疑惑的，为什么会是这个形式？这其实和熵的性质有关，对于熵而言，我们想要这个定义能表示信息，或者说是更直观的不确定性（不确定性越大，表示这个分布所包含的信息量越大）。用更形式化的语言来表示： 1、对于概率密度函数或者概率分布为$p(x)$的一个分布，首先希望熵对应的函数是一个光滑的函数。 2、熵应该具有可加性$$S(x)=\\sum_{x}f(p(x))\\tag{3}$$这样我们变成考虑$f(x)$的形式 3、对于两个独立的随机变量$X$和$Y$，如果概率分布为$p(x)$和$p(y)$，那么由两个变量的独立性知道$p(x,y)=p(x)p(y)$。我们分别观测两个变量得到的信息量应该和同时观测两个变量的信息量相同$S(x+y)=S(x)+S(y)$。 根据性质3可以想到对数函数，为了确定$f(x)$的形式，博客中采用简单的二元分布来推出$f(x)=ax\\log (x)$，然后结合性质2可以知道$$S(x)=\\sum_x ap(x)\\log p(x)\\tag{4}$$ 4、当观测到的事件是一个极不可能发生的事情的时候，我们接收的信息应该很大，如果是一个非常正常的事情，那么信息量应该很小，也就是说熵应该是概率的单调函数。所以公式(4)中选择$a=-1$，这样就得到熵的定义$$S(x)=-\\sum_{x} p(x) \\log p(x)\\tag{5}$$","tags":[{"name":"概率论","slug":"概率论","permalink":"http://weleen.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"name":"最大熵","slug":"最大熵","permalink":"http://weleen.github.io/tags/%E6%9C%80%E5%A4%A7%E7%86%B5/"},{"name":"熵","slug":"熵","permalink":"http://weleen.github.io/tags/%E7%86%B5/"}]},{"title":"Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Pattern","date":"2019-10-21T14:31:29.000Z","path":"2019/10/21/Unsupervised-Cross-dataset-Person-Re-identification-by-Transfer-Learning-of-Spatial-Temporal-Pattern/","text":"CVPR2018 paper，ReID上的transfer learning。 ![note](/img/Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Pattern/note.jpg)","tags":[{"name":"Unsupervised ReID","slug":"Unsupervised-ReID","permalink":"http://weleen.github.io/tags/Unsupervised-ReID/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"http://weleen.github.io/tags/Transfer-Learning/"}]},{"title":"Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification","date":"2019-09-11T21:38:37.000Z","path":"2019/09/12/Self-similarity-Grouping-A-Simple-Unsupervised-Cross-Domain-Adaptation-Approach-for-Person-Re-identification/","text":"ICCV 2019 oral","tags":[{"name":"ReID","slug":"ReID","permalink":"http://weleen.github.io/tags/ReID/"},{"name":"Unsupervised","slug":"Unsupervised","permalink":"http://weleen.github.io/tags/Unsupervised/"},{"name":"Grouping","slug":"Grouping","permalink":"http://weleen.github.io/tags/Grouping/"}]},{"title":"STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification","date":"2019-09-10T14:50:36.000Z","path":"2019/09/10/STA-Spatial-Temporal-Attention-for-Large-Scale-Video-based-Person-Re-Identification/","text":"AAAI2019 paper AbstractIn this work, we propose a novel Spatial-Temporal Attention (STA) approach to tackle the large-scale person re-identification task in videos. Different from the most existing methods, which simply compute representations of video clips using frame-level aggregation (e.g. average pooling), the proposed STA adopts a more effective way for producing robust clip-level feature representation. Concretely, our STA fully exploits those discriminative parts of one target person in both spatial and temporal dimensions, which results in a 2-D attention score matrix via inter-frame regularization to measure the importances of spatial parts across different frames. Thus, a more robust clip-level feature representation can be generated according to a weighted sum operation guided by the mined 2-D attention score matrix. In this way, the challenging cases for video-based person re-identification such as pose variation and partial occlusion can be well tackled by the STA. We conduct extensive experiments on two large-scale benchmarks, i.e. MARS and DukeMTMC-VideoReID. In particular, the mAP reaches 87.7% on MARS, which significantly outperforms the state-of-the-arts with a large margin of more than 11.6%. 问题Video Person ReID，给定probe视频（RGB），排序gallery中的视频。 方法提出了一个新的网络STA来解决video person re-id问题，文章列举的创新点有： 1）对每个spatial region给予权重，这能够同时做到discriminative part mining和frame selection；相比于AAAI2018的Region-based Quality Estimation Network改进了part-based attention，确实在一定程度更加符合现在reid中使用part level特征。这里是不是可以考虑对part特征的选择进行研究，例如使用deformable或者local的attention？不过这个在特征提取过程中已经做了一部分操作，效果不一定有提升。 2）提出一个inter-frame regularization，用来约束不同帧之间需要不类似。 3）新的特征融合方法。 整体看上去并不复杂，方法总览如下： 细节上：1、特征提取：ResNet50的最后一层的stride需要调整成1 这里公式有点问题，文字意思是先对每个point求特征向量的norm的平方，而后在spatial上面做L2 normalization。之后做的L1 normalization和公式也对不上，公式上只是算了每个spatial block的l1 norm。 在得到了每个spatial block的attention score之后，对相同spatial region的score做l1 normalization。 2、regularization的公式，应用在上，注意正则化项只是随机取了两帧计算的，公式在后面。 3、特征融合 使用highest score得到的第一个feature map，使用attention score加权得到第二个feature map，然后使用global average pooling和fc得到最后的特征。 4、算法表 算法表更清晰，但是有一些小错误。 结果从ablation study看出提出的component： STA，Fusion，Reg都能提高结果。","tags":[{"name":"Video Person ReID","slug":"Video-Person-ReID","permalink":"http://weleen.github.io/tags/Video-Person-ReID/"},{"name":"Attention","slug":"Attention","permalink":"http://weleen.github.io/tags/Attention/"}]},{"title":"Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification","date":"2019-09-10T14:40:43.000Z","path":"2019/09/10/Cross-view-Asymmetric-Metric-Learning-for-Unsupervised-Person-Re-identification/","text":"ICCV2017 paper","tags":[{"name":"ReID","slug":"ReID","permalink":"http://weleen.github.io/tags/ReID/"},{"name":"Unsupervised","slug":"Unsupervised","permalink":"http://weleen.github.io/tags/Unsupervised/"},{"name":"Cross-view","slug":"Cross-view","permalink":"http://weleen.github.io/tags/Cross-view/"},{"name":"Metric Learning","slug":"Metric-Learning","permalink":"http://weleen.github.io/tags/Metric-Learning/"}]},{"title":"awesome-reid","date":"2019-07-24T10:11:50.000Z","path":"2019/07/24/awesome-reid/","text":"TOC 1. Survey 2. Person Re-ID 3. Person search 4. MTMC 5. Cross-Modality Re-ID 6. Vehicle Re-ID 7. Other Related Work 8. Datasets 1. Survey A Survey of Open-World Person Re-identification, TCSVT2019, [paper] Person Re-identification: Past, Present and Future, arXiv2016 [paper] A survey of approaches and trends in person re-identification, Image and Vision Computing2014 [paper] Appearance Descriptors for Person Re-identification: a Comprehensive Review, arXiv2013 [paper] 2. Perosn Re-IDUnsupervised Person Re-ID (Domain Adaptation, Semi-Supervised, etc.) Unsupervised Person Re-Identification by Camera-Aware Similarity Consistency Learning, ICCV2019, [paper] Unsupervised Graph Association for Person Re-Identification, ICCV2019, [paper] Instance-Guided Context Rendering for Cross-Domain Person Re-Identification, ICCV2019, [paper] Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification, ICCV2019, oral, [paper] [SSG++] Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification, ICCV2019, [paper][code] A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification, ICCV2019, [paper] One Shot Domain Adaptation for Person Re-Identification, ArXiv2018, [paper] Self-Training With Progressive Augmentation for Unsupervised Cross-Domain Person Re-Identification, ICCV2019, [paper] Weakly Supervised Person Re-Identification, CVPR2019, [paper] Patch-Based Discriminative Feature Learning for Unsupervised Person Re-identification, CVPR2019, [paper] [github] Unsupervised Person Re-identification by Soft Multilabel Learning, CVPR2019, [paper] [github] Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification, CVPR2019, [paper] [github] Adaptive Transfer Network for Cross-Domain Person Re-Identification, CVPR2019, [paper] Domain Adaptation through Synthesis for Unsupervised Person Re-identification, ECCV2018, [paper] Unsupervised Person Re-identification by Deep Learning Tracklet Association, ECCV2018, [paper] Generalizing A Person Retrieval Model Hetero- and Homogeneously, ECCV2018, [paper] [Github] Robust Anchor Embedding for Unsupervised Video Person Re-Identification in the Wild, ECCV2018, [paper] Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns, CVPR2018, [paper] [Github] Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification, CVPR2018, [paper] Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification, CVPR2018, [paper] [Github] Disentangled Person Image Generation, CVPR2018, [paper] Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning, CVPR2018, [paper] [Github] [Homepage] Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification, ICCV2017, [paper] [Github] Efficient Online Local Metric Adaptation via Negative Samples for Person Re-Identification, ICCV2017, [paper] SHaPE: A Novel Graph Theoretic Algorithm for Making Consensus-based Decisions in Person Re-identification Systems, ICCV2017, [paper] Stepwise Metric Promotion for Unsupervised Video Person Re-identification, ICCV2017 [paper] Group Re-Identification via Unsupervised Transfer of Sparse Features Encoding, ICCV2017, [paper] Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro, ICCV2017, [paper] [Github] Dynamic Label Graph Matching for Unsupervised Video Re-Identification, ICCV2017, [paper] [Github] One-Shot Metric Learning for Person Re-Identification, CVPR2017, [paper] Unsupervised Adaptive Re-Identification in Open World Dynamic Camera Networks, CVPR2017, [paper] Unsupervised Person Re-identification: Clustering and Fine-tuning, ArXiv2017, [paper] [Github] Supervised Person Re-ID Co-Segmentation Inspired Attention Networks for Video-Based Person Re-Identification, ICCV2019, [paper] ABD-Net: Attentive but Diverse Person Re-Identification, ICCV2019, [paper] Omni-Scale Feature Learning for Person Re-Identification, ICCV2019, [paper] Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification, ICCV2019, [paper] Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification, ICCV2019, [paper] Mixed High-Order Attention Network for Person Re-Identification, ICCV2019, [paper] Temporal Knowledge Propagation for Image-to-Video Person Re-identification, ICCV2019, [paper] Pose-Guided Feature Alignment for Occluded Person Re-Identification, ICCV2019, [paper] Batch DropBlock Network for Person Re-Identification and Beyond, ICCV2019, [paper] Second-Order Non-Local Attention Networks for Person Re-Identification, ICCV2019, [paper] Global-Local Temporal Representations for Video Person Re-Identification, ICCV2019, [paper] Spectral Feature Transformation for Person Re-Identification, ICCV2019, [paper] Foreground-Aware Pyramid Reconstruction for Alignment-Free Occluded Person Re-Identification, ICCV2019, [paper] Self-Critical Attention Learning for Person Re-Identification, ICCV2019, [paper] Temporal Knowledge Propagation for Image-to-Video Person Re-Identification, ICCV2019, [paper] Deep Constrained Dominant Sets for Person Re-Identification, ICCV2019, [paper] Densely Semantically Aligned Person Re-Identification, CVPR2019, [paper] Generalizable Person Re-identification by Domain-Invariant Mapping Network, CVPR2019, [paper] Re-Identification with Consistent Attentive Siamese Networks, CVPR2019, [paper] Distilled Person Re-identification: Towards a More Scalable System, CVPR2019, [paper] Towards Rich Feature Discovery with Class Activation Maps Augmentation for Person Re-Identification, CVPR2019, [paper] AANet: Attribute Attention Network for Person Re-Identification, CVPR2019, [paper] Pyramidal Person Re-IDentification via Multi-Loss Dynamic Training, CVPR2019, [paper] Interaction-and-Aggregation Network for Person Re-identification, CVPR2019, [paper] Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification, CVPR2019, [paper] VRSTC: Occlusion-Free Video Person Re-Identification, CVPR2019, [paper] Attribute-Driven Feature Disentangling and Temporal Aggregation for Video Person Re-Identification, CVPR2019, [paper] Maximum Margin Metric Learning Over Discriminative Nullspace for Person Re-identification, ECCV2018, [paper] Person Re-identification with Deep Similarity-Guided Graph Neural Network, ECCV2018, [paper] Pose-Normalized Image Generation for Person Re-identification, ECCV2018, [paper] Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association, ECCV2018, [paper] Hard-Aware Point-to-Set Deep Metric for Person Re-identification, ECCV2018, [paper] Part-Aligned Bilinear Representations for Person Re-Identification, ECCV2018, [paper] [Github] Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-identification, ECCV2018, [paper] Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline), ECCV2018, [paper] Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification, CVPR2018, [paper] A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking, CVPR2018, [paper] Human Semantic Parsing for Person Re-identification, CVPR2018, [paper] Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding, CVPR2018, [paper] Mask-guided Contrastive Attention Model for Person Re-Identification, CVPR2018, [paper] Person Re-identification with Cascaded Pairwise Convolutions, CVPR2018, [paper] Multi-Level Factorisation Net for Person Re-Identification, CVPR2018, [paper] Attention-Aware Compositional Network for Person Re-identification, CVPR2018, [paper] Deep Group-shuffling Random Walk for Person Re-identification, CVPR2018, [paper] Harmonious Attention Network for Person Re-Identification, CVPR2018, [paper] Efficient and Deep Person Re-Identification using Multi-Level Similarity, CVPR2018, [paper] Pose Transferrable Person Re-Identification, CVPR2018, [paper] Adversarially Occluded Samples for Person Re-identification, CVPR2018, [paper] Camera Style Adaptation for Person Re-identification, CVPR2018, [paper] Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification, CVPR2018, [paper] Easy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference Constraints, CVPR2018, [paper] Eliminating Background-bias for Robust Person Re-identification, CVPR2018, [paper] Features for Multi-Target Multi-Camera Tracking and Re-Identification, CVPR2018, [paper] Multi-shot Pedestrian Re-identification via Sequential Decision Making, CVPR2018, [paper] End-to-End Deep Kronecker-Product Matching for Person Re-identification, CVPR2018, [paper] Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-free Approach, CVPR2018, [paper] Resource Aware Person Re-identification across Multiple Resolutions, CVPR2018, [paper] Group Consistent Similarity Learning via Deep CRF for Person Re-Identification, CVPR2018, [paper] A Two Stream Siamese Convolutional Neural Network For Person Re-Identification, ICCV2017, [paper] Learning View-Invariant Features for Person Identification in Temporally Synchronized Videos Taken by Wearable Cameras, ICCV2017, [paper] Deeply-Learned Part-Aligned Representations for Person Re-Identification, ICCV2017, [paper] Pose-driven Deep Convolutional Model for Person Re-identification, ICCV2017, [paper] Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification, ICCV2017, [paper] Multi-scale Deep Learning Architectures for Person Re-identification, ICCV2017, [paper] Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification, CVPR2017, [paper] Multiple People Tracking by Lifted Multicut and Person Re-Identification [paper] Pose-Aware Person Recognition [paper] Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-Identification, CVPR2017, [paper] Spindle Net: Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion, CVPR2017, [paper] Re-Ranking Person Re-Identification With k-Reciprocal Encoding, CVPR2017, [paper] Scalable Person Re-Identification on Supervised Smoothed Manifold, CVPR2017, [paper] Point to Set Similarity Based Deep Feature Learning for Person Re-Identification, CVPR2017, [paper] Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation, CVPR2017, [paper] See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification, CVPR2017, [paper] Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network, CVPR2017, [paper] Recurrent Convolutional Network for Video-based Person Re-Identification, CVPR2016, [paper] Joint attentive spatial-temporal feature aggregation for video-based person re-identification”, IEEE Access [paper] Other Methods advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns, ICCV2019, [paper] SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification, ICCV2019, [paper] Joint Discriminative and Generative Learning for Person Re-identification, CVPR2019, [paper] [github] Progressive Pose Attention Transfer for Person Image Generation, CVPR2019, [paper] [github] Unsupervised Person Image Generation with Semantic Parsing Transformation, CVPR2019, [paper] [github] Learning to Reduce Dual-level Discrepancy for Infrared-Visible Person Re-identification, CVPR2019, [paper] Re-ranking via Metric Fusion for Object Retrieval and Person Re-identification, CVPR2019, [paper] Integrating Egocentric Videos in Top-view Surveillance Videos: Joint Identification and Temporal Alignment, ECCV2018, [paper] Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification, ECCV2018, [paper] Adversarial Open-World Person Re-Identification, ECCV2018, [paper] Exploiting Transitivity for Learning Person Re-identification Models on a Budget [paper] 3. Person Search Learning Context Graph for Person Search, CVPR2019, [paper] [github] Query-guided End-to-End Person Search, CVPR2019, [paper] [github] RCAA: Relational Context-Aware Agents for Person Search, ECCV2018, [paper] Person Search in Videos with One Portrait Through Visual and Temporal Links, ECCV2018, [paper] Person Search by Multi-Scale Matching, ECCV2018, [paper] Person Search via A Mask-Guided Two-Stream CNN Model, ECCV2018, [paper] Neural Person Search Machines, ICCV2017, [paper] Person Search with Natural Language Description, CVPR2017, [paper] Joint Detection and Identification Feature Learning for Person Search, CVPR2017, [paper] 4. MTMC5. Cross-Modality Re-ID Text Guided Person Image Synthesis, CVPR2019, [paper] Re-Identification Supervised Texture Generation, CVPR2019, [paper] RGB-Infrared Cross-Modality Person Re-Identification, ICCV2017, [paper] 6. Vehicle Re-ID A Dual-Path Model With Adaptive Attention For Vehicle Re-Identification, ICCV2019, [paper] Part-regularized Near-Duplicate Vehicle Re-identification, CVPR2019, [paper] Viewpoint-aware Attentive Multi-view Inference for Vehicle Re-identification, CVPR2018, [paper] Orientation Invariant Feature Embedding and Spatial Temporal Regularization for Vehicle Re-identification, ICCV2017, [paper] 7. Other Related Work Person Re-identification by Video Ranking, ECCV2014 [paper] Person Re-identification using View-dependent Score-level Fusion of Gait and Color Features, ICPR 2012 [paper] Enhancing Person Re-identification by Integrating Gait Biometric, Neurocomputing 2015 [paper] A Hierarchical Method Combining Gait and Phase of Motion with Spatiotemporal Model for Person Re-identification, Pattern Recognition Letters 2012 [paper] Person Re-Identification by Discriminative Selection in Video Ranking, TPAMI 2016 [paper] A Spatio-temporal Appearance Representation for Video-based Pedestrian Re-identification, ICCV 2015 [paper] [project] Gait-Assisted Person Re-identification in Wide Area Surveillance, ACCV 2014 [paper] Person Re-identiﬁcation by Unsupervised Video Matching, Pattern Recognition 2017 [paper] Swiss-System Based Cascade Ranking for Gait-Based Person Re-identification, AAAI 2015 [paper] Person Re-identification using Height-based Gait in Colour Depth Camera, ICIP 2013 [paper] Review of person re-identification techniques, IET Computer vision 2013 [paper] Person Re-identification in Appearance Impaired Scenarios, arXiv 2016 [paper] [github] Learning Compact Appearance Representation for Video-based Person Re-Identification, arXiv 2017 [paper] Recurrent Attention Models for Depth-Based Person Identification, CVPR 2016 [paper] [github] Towards view-point invariant Person Re-identification via fusion of Anthropometric and Gait Features from Kinect measurements, VISAPP 2017 [paper] Person Re-identification in Frontal Gait Sequences via Histogram of Optic Flow Energy Image, ICACIVS 2016 [paper] Learning Bidirectional Temporal Cues for Video-based Person Re-Identification, TCSVT 2016 [paper] Gait-based Person Re-identification, A Survey, ACM CSUR 2019 [paper] 8. Datasets CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification, CVPR2019, [paper] VERI-Wild: A Large Dataset and a New Method for Vehicle Re-Identification in the Wild, CVPR2019, [paper] Dissecting Person Re-identification from the Viewpoint of Viewpoint, CVPR2019, [paper] [github] Person Transfer GAN to Bridge Domain Gap for Person Re-Identification, CVPR2018, [paper] Person Re-Identification in the Wild, CVPR2017, [paper] [Market-1501 Leaderboard][Collection]Linksthis repo refers to handong and bismex.","tags":[{"name":"ReID","slug":"ReID","permalink":"http://weleen.github.io/tags/ReID/"}]},{"title":"S+U images Adversial Training","date":"2017-06-27T12:23:13.000Z","path":"2017/06/27/S-U-images-Adversial-Training/","text":"CVPR2017论文 Learning from Simulated and Unsupervised Images through Adversarial Training 文章主要提出的几个新idea： 用无标签的图像以及合成图像进行学习，让图像更真实 训练一个refine网络对合成图像进行对抗学习 对GAN进行了一些修改，让训练更稳定，主要是后文提到的history和local patch S+U Learning 整个框架还是沿用的GAN的架构，G网络：用simulator合成的图像作为refine network的输入。D网络：把前面的refine network的输出和unlabelled图片作为输入，输出判别是真是图片还是虚假图片。 G网络的loss在GAN的基础上增加了一个regularization loss，主要是因为G网络的输入是带标签的合成数据（比如使用的gaze image，是有gaze direction的，而整个实验最后还是为了用来提高监督学习的效果）。loss表示成$$L_R(\\theta) = \\sum_{i} l_{real}(\\theta; \\hat{x}i, y) + \\lambda l{reg}(\\theta;\\hat{x}i, x_i)$$，其中第一项loss是对抗loss，即$l{real}(\\theta;\\hat{x}i,y)=-\\sum{log(1-D{\\phi}(R_{\\theta}(x_i)))}$，$D_{\\phi}$是判别器的输出，表示图片是合成图片的概率，可以发现这个loss越小，那么合成图像越难以分辨（$1-D_{\\phi}(R_{\\theta}(x_i))$越大）。$ l_{reg}=||R_{\\theta}(x_i)-x_i||_1 $使得输入和输出相似。而判别器的loss就是GAN的loss，这里就不写出来了。 改进主要的改进点在最开始的地方已经写过了，一个是local patch，一个是history，下面分别介绍。 local patch adversarial loss因为GAN在训练的时候会出现artifacts的情况，具体来说就是图片没有意义，但是loss很小，原因应该是学习的feature是判别器不容易判别的feature。讲artifects的文章可以看看distill。为了解决这个问题，作者提出用local patch作为单位（对判别器而言），对原始图像最后卷积成w x h的图像，w和h是图像被分成的patch大小，然后判别器的对抗loss变成交叉熵loss的和(The adversarial loss function is the sum of the cross-entropy losses over the local patches. 这段看原文可能更清楚一点)。 history这个方法主要是为了应对两个问题： 在对抗学习过程中，因为不同的训练样本服从的分布还是有区别的，如果仅仅使用refine network最近的输出作为输入，那么容易出现divergency。 refine network可能会产生判别器遗忘的artifects。第一个问题，我认为不解决的话需要调超参，比较麻烦，但是通过引入历史的refined images能提高多少，我觉得效果应该也不会很好，毕竟合成图像也是随机生成的。第二个问题应该能够有很大的改善。具体实现就是保存一个refined images的池，在每次训练判别器的时候，minibatch输入有一部分是从池中选取，一部分是G网络产生。 实验实验用了两个数据集，一个是MPIIGaze，一个NYU Hand。先是展示了一下生成器生成图像的真实性，用了一个表还有几张图片，可以看出生成图片还是很逼真的。 具体实验结果就不放了，大家可以直接去看论文。","tags":[{"name":"GAN","slug":"GAN","permalink":"http://weleen.github.io/tags/GAN/"}]},{"title":"Bootstrapping Face Detection with Hard Negtive Examples","date":"2016-08-18T13:35:57.000Z","path":"2016/08/18/Bootstrapping-Face-Detection-with-Hard-Negtive-Examples/","text":"最近在做一个detection的项目，所以找了很多相关的文章。其中这篇文章提到的Hard Negtive Mining能够解决在实际工程里面出现误检的现象，另外文章中使用的resNet准备后面也尝试一下，感觉性能会比现在修改了的googleNet更强。 文章内容文章一共就7页，主要讲的就是基于faster rcnn做的一个人脸检测的方法， faster rcnn分成rpn和fast rcnn两个部分。 Faster-RCNN文章介绍的Hard Negtive Mining可以理解为一种训练方法，在faster rcnn正常训练的过程中，rpn输出了预测的一系列框和打分(proposals)，在这些proposals中，有正确的框和错误的框(positive samples和negtive samples)。有了这些proposals，fast rcnn的训练就应该可以进行了呗。但是细节上处理的时候，正样本和负样本的比例被设置成1:3，这里应该是防止训练过程中出现太多负样本导致训练出问题(比如proposals里面有一个正样本，剩下全是负样本，训练的时候提的特征都会是和你需要检测的目标没有关系的特征)。这样就有一个选择负样本的问题了，在rbg的py-faster-rcnn中是随机选择一些负样本的。这篇文章就希望在这个地方做点改进，通过修改负样本让训练的网络更加具有判别性(deterministic)。 Hard Negtive Miningreddit上有个讲解通俗易懂Hard Negtive Mining。大致意思就是说：训练完之后，在训练样本中有那么一些训练数据输入网络中输出的proposal和score是有问题的(比如在行人检测中，提取的proposal没有框到人，但是打分却很高)，这样的proposal被称为false positive，可以保存下来作为训练的时候的负样本。所以整个改进的训练流程就变成这样子：1.第一次训练和之前一样，得到一个模型；2.用训练好的模型以及测试网络，对每张训练图片跑一边，会得出一系列的proposal，拿这些proposal和gt对比，如果proposal的打分大于一定值而且和gt的IoU又小于一个阈值，就把这个proposal看作false positive；3.修改训练的网络，把保存的false positive放到训练的时候用的negtive samples里面去，完成训练。 总结总体来说方法比较简单，修改的文件内容也不是很多，提取false positive用到demo.py，修改网络修改roi_data_layer/minibatch.py和roi_data_layer/layer.py还有rpn/proposal_target_layer.py三个文件。最后说一下实验结果可以看到左上角的两条线分别是resNet和resNet with HNM，提高是有的，不过基础的resNet就已经比faster rcnn文章里面用的VGG16好了很多了(VGG16应该是最后高了70%一点吧)，所以说resNet和这个比起来还是更直接一点。","tags":[{"name":"DNN","slug":"DNN","permalink":"http://weleen.github.io/tags/DNN/"},{"name":"Hard Negtive Mining","slug":"Hard-Negtive-Mining","permalink":"http://weleen.github.io/tags/Hard-Negtive-Mining/"}]},{"title":"awesome hand pose estimation","date":"2016-07-21T14:48:43.000Z","path":"2016/07/21/Hand Pose Estimation/","text":"depth based hand pose estimation相关资源 Homepage Tang Philip Krejov CVARlab MPIhand Tracking Design Lab Dataset Hands2015 VIVA NYU ICVL","tags":[{"name":"Hand Pose Estimation","slug":"Hand-Pose-Estimation","permalink":"http://weleen.github.io/tags/Hand-Pose-Estimation/"}]},{"title":"Install Caffe on ubuntu14.04","date":"2016-07-21T12:39:21.000Z","path":"2016/07/21/Install-Caffe-on-ubuntu14-04/","text":"服务器前段时间挂了一直没有安装，最近GPU不够用了，又要把服务器拿来重新装了。之前一直用的Centos7（感觉用图形界面经常会出问题），现在换ubuntu14.04。先写个开头，装完了补上。 参考资料： .run安装和官方教程 ubuntu和cuda的安装包下载就不多说，ubutnu下载完之后，解压到U盘里面，在服务器选择启动项的时候选UEFI启动（UEFI启动才能用直接解压的ubuntu安装包，而且ubuntu12前的版本好像是不能这么装的），具体的google应该可以找到。 假设现在装完了ubuntu，cuda的安装包(.run离线安装)下载完了，我下载的是cuda7.5（这个版本的编译caffe的时候可能会出现caffe提示找不到libcudart.so.7.0，解决办法直接软链接一个libcudart.so.7.0到7.5版本的so文件）。","tags":[{"name":"Linux","slug":"Linux","permalink":"http://weleen.github.io/tags/Linux/"}]},{"title":"Understanding-and-Diagnosing-Visual-Tracking-Systems","date":"2016-06-13T20:36:24.000Z","path":"2016/06/14/Understanding-and-Diagnosing-Visual-Tracking-Systems/","text":"ICCV15文章，主要关注short-term single-object model-free tracking，文章主要是想探究现在在benchmark上进行的实验是不是足以证明跟踪器的好坏。 short-term:在目标跟丢了的情况下跟踪器不会继续跟踪。single-object:不解释了。model-free:在跟踪过程中只有第一帧有一个训练样本。名词解释可以看VOTchallenge2014的paper。 作者把整个跟踪过程分成五个部分来考虑：Motion Model, Feature Extractor, Observation Model, Model Updater, Ensemble Post-processor. 对比实验的评价标准就是AUC和precision plot。 Validation and Analysis文章从上面分开的五个部分分别对比了各个部分中经常使用的设置，并且做了分析。 Feature Extractor特征的选择上面也会导致结果有很大的不同，文章里面比较了五个常用的特征，还提到CNN特征可以用，效果很好但是跟踪的速度慢。作者给出的总结：The feature extractor is the most important component of a tracker. Using proper features can dramatically improve the tracking performance. Developing a good and effective feature representation for tracking is still an open problem. Observation Model前面作者把Observation Model分成了generative model和discriminative model，前者（产生式的过程）的代表是PCA、sparse coding、dictionary learning，后者（分类器）的代表有boosting、structured output SVM、deep learning。文章说大部分人使用的是discriminative model，所以只分析了这个。对比了logistic regression、ridge regression、SVM、Structured output SVM。对比结果显示Observation model的选择和特征有很大的关系，在用raw pixel和HOG特征的时候结果有很大差距。作者的总结：Different observation models indeed affect the performance when the features are weak. However, the performance gaps diminish when the features are strong enough. Consequently, satisfactory results can be obtained even using simple classifiers from textbooks. Motion ModelMotion Model产生一系列的candidate，文章对比了粒子滤波、滑动窗、半径滑动窗，结果显示粒子滤波的效果最好，但是差距不是大。另外还考虑了fast motion和scale variation的情况，得到将video resize到固定大小能够提升效果的结果。作者的结论：When compared to the feature extractor and observation model components, in general the motion model only has minor effects on the performance. However, under scale variation and fast motion, setting the parameters properly is still crucial to obtaining good performance. Furthermore, for some specific scenarios such as egocentric video, it is beneficial to design the motion model carefully. Due to its ability to adapt to scale changes which are not uncommon in practice, we will still take the particle filter approach with resized input as the default motion model in the sequel. Model Updater这部分不是很懂实验设计的理由，用两种不同的更新策略做对比。作者的总结：Although implementation of the model updater is often treated as engineering tricks in papers especially for discriminative trackers, their impact on performance is usually very significant and hence is worth studying. Unfortunately, very few work focuses on this component. Ensemble Post-processor作者总结：The ensemble post-processor can improve the performance substantially especially when the trackers have high diversity. This component is universal and effective yet it is least explored. ConclusionFeature Extractor最重要，其次是Observation Model，然后model updater也能影响结果，但是现行的算法大多不注重这个点的提升，Ensemble Post-processor比较普遍，没有很多人关注这个点，至于Motion Model，如果在这上面采取合适的修改也可以得到不错的效果。","tags":[{"name":"Object Tracking","slug":"Object-Tracking","permalink":"http://weleen.github.io/tags/Object-Tracking/"}]},{"title":"Online Object Tracking: A Benchmark","date":"2016-06-13T15:19:01.000Z","path":"2016/06/13/Online-Object-Tracking-A-Benchmark/","text":"13年的CVPR文章，一个online object tracking的文章，介绍了很多比较新的算法，给了算法的code和一个benchmark。download Introduction目标跟踪的过程：给定一个初始状态，在随后的帧中跟踪这个目标。没有一个固定的方法可以应对所有的场景，所以不同的算法一般先验不同。说明了一下自己文章的三个贡献： 发布了一个数据集 给了29个算法的源码 测试了算法的鲁棒性 Related Work Representation Scheme，大致说了有原始图像的光照强度值、子空间方法使用的appearance representation还有sparse representation以及特征点。 Search Mechanism，有确定性的方法，随机的方法。比如在转化成优化问题的时候，如果问题有最优解就直接可以找到最好的跟踪方法。但是大多数优化问题是非线性而且有很多局部最优解的，这个时候就有可能用采样的方法，但是这个方法代价大。随机的方法作者举了粒子滤波。 Model Update，在跟踪过程中实时更新模型可以更好得跟踪目标，文章提了些更新算法。也说到现在的appearance model还是不能做到不drift。 Context and Fusion of Trackers，说上下文信息和融合算法能提高算法的准确度。 Evaluation and Datasets文章给出了最近一些算法的比较，在representation、search domain、model update、code、FPS进行了比较。还说了自己的Dataset比之前的好。另外，作者分析了Dataset的一些属性，比如里面有多少帧是出现旋转的、出现模糊、出现遮挡的。 Evaluation Methodolgy评判的准则：Precision plot（用ground-truth和跟踪结果中心点的pixel距离表示跟踪正确与否，文中用20pixel作为阈值）、Success plot（用boundingbox overlap=$\\frac{交集面积}{并集面积}$绘制Success plot，用AUC来作为跟踪算法排序的标准）、Robustness Evaluation（因为初始化不同可能跟踪效果差距比较大，所以用两种方法1、在不同的帧进行初始化，TRE（temporal robustness evaluation）；2、初始化是用不同的boundingbox（spatial robustness evaluation））。 Evaluation Results做了各种实验，对比了各种实验结果。有兴趣可以仔细看看对比实验，不一样的算法在不同条件下有不同的performance。 Conclusion 背景信息在跟踪过程中还是很重要的，比如Struck和CXT； 局部特征和全局特征相比，局部特征在目标出现遮挡还有形变的时候会更好； 运动模型和动态模型对跟踪很重要。","tags":[{"name":"Object Tracking","slug":"Object-Tracking","permalink":"http://weleen.github.io/tags/Object-Tracking/"}]},{"title":"Object Tracking: A Survey","date":"2016-06-05T13:33:17.000Z","path":"2016/06/05/Object-Tracking-A-Survey/","text":"这篇文章是一篇经典的有关目标跟踪的review paper，由于最近在看相关的文献，把这篇文章找出来看一看，顺便写写相关的总结。download Introduction目标跟踪是计算机视觉领域重要的一个任务，在视频分析中有三个重要步骤：感兴趣的运动物体检测，运动物体的跟踪以及根据运动分析行为。目标跟踪的应用点有基于运动的目标识别，智能监控，视频索引，人机交互，交通监控，车辆导航。 目标跟踪问题面临的挑战包括：3D物体向2D投影的信息损失，图像噪声，目标运动复杂，非刚性物体的跟踪，目标会出现遮挡，目标形状复杂，场景光照变换以及实时跟踪的需求。 目标跟踪问题一般都会有一定的假设来简化问题，比如假设目标运动是平滑的或者加速运动是均匀的，或者目标的数量是确定的。 Object Representation目标表示形式有很多种： Shape Representations 点，把目标表示成一个点或者一系列点，这种方法适用于跟踪小范围的目标。 几何形状， 把目标表示成一个长方形或者是椭圆等等，这种时候，目标运动就可以表示成这种几何形状的平移、旋转、仿射变换等等。这种表示方式适合简单的刚性物体，也可以用于跟踪非刚性物体。 目标的轮廓，用目标轮廓来确定目标的范围，在轮廓内部的称为目标的剪影，这种表示方法适合于简单的非刚性物体。 铰链式的形状模型，通过把目标分成一个个部分，连接各个部分来表示一个目标，比如人体就可以分成头、胳膊、手掌、脚等部分，而各个部分之间通过运动模型来相互关联。 骨骼模型，目标的骨骼模型可以用于建模有关节的和刚性目标。 Appearance Representations Probability densities of object appearance，The probability densities of object appearance features (color, texture) can be computed from the image regions specified by the shape models (interior region of an ellipse or a contour). Template，用简单的几何形状或者是剪影合成Template，但是Template只适合目标姿势变化不是很大的情况。 Active appearance models, 对目标形状和外观同时建模可以得到Activce appaerance models， 目标的形状一般是a set of landmarks，对于每一个landmark，表示成颜色、纹理或是梯度的一个向量。 Multiview appearance models，组合多个角度的目标信息。 One approach is to generate a subspace from the given views， 可以通过PCA和ICA实现。 Another approach is by training a set of classifiers。 Feature selection for tracking最佳的特征是可以将目标很容易分辨出来的特征，特征选择和目标的表示密切相关，例如在使用直方图表示目标时，一般使用颜色特征；在使用目标的轮廓来表示目标时，一般使用目标的角点来作为特征。 颜色， 有RGB、LUV、LAB、HSV等颜色空间可以选择，但是颜色特征比较容易受到噪声影响。 边，边的一个重要性质就是不易受到光照变化的影响，而且比较容易实现。 光流，光流假定光照不变，定义了区域内每个像素的位移量。一般光流用在分割和跟踪问题中。 纹理，纹理用来表示物体表面的平滑性和规则性。和颜色特征相比，产生纹理特征需要多一步处理的过程。和边特征类似，纹理特征也对光照变化比较鲁棒。 Object Detection目标跟踪需要一个目标检测工具，一般的目标检测算法使用单帧中的信息，也有一些算法使用时间信息来减少错误检测，比如计算连续帧之间的差值，在连续的帧之中高亮那些变化的区域。 Point Detectors点检测器有Harris、KLT、SIFT等。一篇关于点检测器的论文A Performance Evaluation of Local Descriptors。Harris角点用了一阶微分作为特征$(I_x,I_y)$，用下面的式子表示一个pixel的值。详细的Harris角点的资料可以在网上找找。还有KLT角点和SIFT角点也很常用。 Background Subtraction背景减除的过程描述：建立背景模型，对于每帧计算偏差，变化较大的区域就是移动的目标。可以用GMM对每个像素点建模，分离前景和背景。还可以结合其他信息，比如纹理信息，以应对光照变换等情况。还有使用HMM建模，用特征值分解等方法。以上的这些方法都是针对静止背景的。对于Time-vary background，有一种ARMA的模型可以用来解决。 Segmentation分割的任务是把图片分成一些类似的区域。对分割算法，最重要的是分割结果的评价和分割的方法。 Mean-shift Clustering用mean-shift分割的思想来进行tracking。 Image Segmentation Using Graph-Cuts用过minimum cut来做segmentation，graph的weight是根据color similarity决定的，这个方法的问题在于可能出现oversegmentation， Normailized cut可以解决这个问题。 Active Contour对一个能量函数的优化得到目标区域的轮廓。 Supervised Learning用监督学习的方式学习分类器，将目标和非目标的物体分离开，有很多样的方法，神经网络、决策树、adaboost、SVM等。对于数据那个小的问题，可以用联合学习的方式解决。文章详细介绍了adaboost和SVM。 Object Tracking Point Tracking，目标用点表示，这种方法需要另外的检测方法。 Kernel Tracking，Kernel refers to the object shape and appearance。目标跟踪一般通过带参的形变，比如平移、旋转和仿射变换。 Silhouette Tracking，估计目标区域来进行跟踪，一般用区域内的信息，表示形式有appearance density和shape models。 Point Tracking Methods Kernel Trackinginlcude two subcategories: templates and density-based appearance models, and multiview appearance models. Tracking Using Template and Density-Based Appearance Models single object, 1)template matching, a brute force search method.由于搜索代价比较大，所以一般搜索空间限定在上一帧位置附近。2）mean-shift。3）多个模块结合的跟踪器。4）光流法。 multiple objects, 1)Tao et al. [2002] propose an object tracking method based on modeling the whole image as a set of layers。为整个图像建立多层模型，背景和各个目标都有自己的模型。2）Isard and MacCormick [2001] propose joint modeling of the background and foreground regions for tracking，用混合高斯模型作为前景和背景的模型。 Tracking Using MultiviewAppearance Models上面的方法是用直方图、templates这样的模型来表示目标，在相邻帧之间如果出现比较大的变化，模型可能就不可靠了。使用离线学习的multiview的模型可以克服这个缺点。文章举了两个例子，一种方法是用PCA，一种是用SVM。 Silhouette Tracking不能用一般的形状表示的物体，可以用Silhouette来精确刻画。可以把Silhouette tracking分为shape matching approaches和contour tracking approaches。","tags":[{"name":"Object Tracking","slug":"Object-Tracking","permalink":"http://weleen.github.io/tags/Object-Tracking/"}]},{"title":"A Survey of Appearance Models in Visual Object Tracking","date":"2016-05-31T09:56:19.000Z","path":"2016/05/31/A-Survey-of-Appearance-Models-in-Visual-Object-Tracking/","text":"最近看的一篇综述论文 http://arxiv.org/abs/1303.4803 What’s difficult in this research?研究的难点？Handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion.目标可能由于旋转、遮挡、形变的原因导致外形变化。 Introductionappearance modelingThe most important issue is effective modeling of the 2D appearance of tarcked objects.主要需要解决的是对2维目标的外观建模，分解成了视觉表达和统计建模。The appearance modeling is decomposed into this two stage: visual representation statistical modeling overview of object trackingThe typical object tracking system is composed of four modules Object Initialization. manual or automatic, manual is performed by user to annotate the object, automatic is achieved by object detectors. Appearance Modeling. It consists of two components: visual representation and statistical modeling. Motion Estimation. This is formulated as a dynamic state estimation problem:$x_t = f(x_{t-1},v_{t-1})$ and $z_t = h(x_t,w_t)$where $x_t$ is the current state, $f$ is the state evolution function, $v_{t−1}$ is the evolution process noise, $z_t$ is the current observation, $h$ denotes the measurement function, and $w_t$ is the measurement noise. Object Localization. 整个目标跟踪系统被分成：1、目标初始化；2、外观建模；3、运动预测；4、目标定位。 The Orgnization of the survey likes the figure under. Contribution Review the literature of visual representations from a feature-construction viewpoint. Categorize the representation into local and global features. Classify the existing model into generative, discriminative, and hybrid generative-discriminative. Give some benchmark resources. 贡献如上 Visual RepresetationRepresentation is classified into local and global to introduce.视觉表达可以分成全局和局部。 GlobalRaw Pixel RepresentationSimple and efficient, construct in vector-based or matrix-based. Optical Flow RepresentationIt has two branches: constant-brightness-constraint optical flow and non-brightness-constraint optical flow. Histogram Representationsingle-cueencode the information in the object region. multi-cueencode more information to enhance the robusness of visual represetation. a. Spatial Colorjoint spatial-color modeling(describe the distribution properties of object appearance in a joint spatial-color space) and patch-division. b. Spatial TextureAn estimate of the joint spatial-texture probability is made to capture the distribution information on object appearance. c. Shape Texture Covariance Representationsthe covariance matrix representations can be divided into two branches: affine-invariant Riemannian metric-based and log-Euclidean Riemannian metric-based. The affine-invariant Riemannian metric $$\\rho(\\textbf{C}1,\\textbf{C}_2)=\\sqrt{\\sum{j=1}^d ln^2\\lambda_j(\\mathbf{C}_1,\\textbf{C}_2)}$$ The log-Euclidean Riemannian metric $$d(C_i,C_j)=||\\log (C_i,C_j)||$$ Wavelet Filtering-Based Representationa wavelet filtering-based representation takes advantage of wavelet transforms to filter the object region in different scales or directions Active Contour RepresentationIn order to track the nonrigid objects, active contour representations have been widely used. Discussion原始的pixel特征简单，但是只考虑了颜色信息，不能克服光照变化。 在亮度不变的前提下，CBC光流能包含场信息，但是假设是亮度不变，所以不适应有噪点、光照变化和局部形变，NBC光流由此改进。 single-cue直方图作为统计特征，会失去目标的空间特性，一般会收到背景的影响，spatial-color直方图可以结合空间特性，shape-texture直方图包含形状和纹理信息，可以克服光照变化和姿势变化。 互相关矩阵有以下优点： 能使用目标表观的互相关性； 能够融合不同模态的图像特征； 维度低，计算高效； 能对比不同大小的区域； 比较容易实现； 光照不变、抗形变和遮挡。 缺点有： 容易受噪声影响； 丢失了很多信息，比如纹理、形状、位置。 小波滤波通过小波变换结合了局部的纹理信息，多尺度多方向表达了目标的统计特性。 活动轮廓可以解决非刚性物体的跟踪。 Local Feature-Based Visual RepresentationClassify into seven classes: local template-based, segmentation-based, SIFT-based, MSER-based, SURF-based, corner feature-based, feature pool-based, and saliency detection-based Local Template-BasedRepresent an object region using a set of part templates. Segmentation-BasedA segmentation-based visual representation incorporates the image segmentation cues SIFT-basedMaking use of the SIFT features inside an object region to describe the structural information of ob-ject appearance. MSER-BasedAn MSER-based visual representation needs to extract the MSER (maximally stable extremal region) features for visual representation SURF-Basedthe SURF (Speeded Up Robust Feature) is a variant of SIFT Corner Feature-BasedLocal Feature-Pool-BasedSaliency Detection-BasedSaliency detection is inspired by the focus-of-attention (FoA) theory Discussion基于局部模板的表达，能够应对遮挡问题。基于分割的表达可以得到目标的结构化特征（例如目标边缘和超像素）。基于SIFT特征的表达对光照、形变和遮挡有鲁棒性，但是不能包含目标的大小，方向和姿势等特征。基于MSER（最大稳定极值区域）的表达可以容忍噪音但是不能克服光照变化。基于SURF特征的表达具有尺度不变、旋转不变和计算高效的优点。基于角点的特征表达对有大量角点的目标比较适用，但是不能应对形变和噪声。基于池化的特征表达。基于Saliency的特征表达。 Statistical Modeling For Tracking-By-DetectionThe statistical modeling is classified into three categories, including generative, discriminative, and hybrid generatice-discriminative. The generative appearance models mainly concentrate on how to accurately fit the data from the object class, but suffer from distractions caused by the background regions with similar appearance to the object class. Discriminative appearance models pose visual object tracking as abinary classification issue. They aim to maximize the separability between the object and non-object regions discriminately. A major limitation of the discriminative appearance models is to rely heavily on training sample selection. Due to taking a heuristic fusion strategy, HGDAMs(hybrid generative-discriminative appearance models) cannot guarantee that the performance of the hybrid models after information fusion is better than those of the individual models. Mixture Generative Appearance ModelsThis type of generative appearance model adaptively learns several compo- nents for capturing the spatiotemporal diversity of object appearance containing WSL mixture models and Gaussian mixture models. WSL Mixture Modelsthe WSL mixture model contains the following three components: W-component, S-component, and L- component.Robust Online Appearance Models for Visual Tracking andVisual Tracking and Recognition Using Appearance-Adaptive Models in Particle Filters. Gaussian Mixture ModelsIn essence, the Gaussian mixture models utilize a set of Gaussian distributions to approximate the underlying density function of object appearance. Kernel-Based Generative Appearance ModelsKernel-based generative appearance models(KGAMs)utilize kernel density estimation to construct kernel-based visual representations and then carry out the mean shift for object localization. Including color-driven KGAMs, shape-integration KGAMs, scale-aware KGAMs, nonsymmetric KGAMs, KGAMs by global mode seeking, and sequential-kernel-learning KGAMs. Subspace Learning-Based Generative Appearance ModelsAccording to the used techniques for subspace analysis, they can be categorized into two types: conventional and unconventional SLGAMs. Boosting-Based Discriminative Appearance ModelsAccording to the learning strategies employed, they can be categorized into self-learning and co-learning BDAMs. Self-learning-based BDAMs adopt the self-learning strategy to learn object/non-object classifier.In order to tackle this problem, co-learning-based BDAMs are developed to capture the discriminative information from many unlabeled samples in each frame. SVM-Based Discriminative Appearance ModelsSDAMs aim to learn margin-based discriminative SVM classifiers for maximizing in- terclass separability. SDAMs are typically based on self-learning SDAMs and co-learning SDAMs. Randomized Learning-Based Discriminative Appearance ModelsIn principle, randomized learning techniques can build a diverse classifier ensemble by performing random input selec- tion and random feature selection. Discriminant Analysis-Based Discriminative Appearance ModelsIn principle, its goal is to find a low-dimensional subspace with a high interclass separability. According to the learning schemes used, it can be split into two branches: conventional discriminant analysis and graph-driven discriminant analysis. However, a drawback is that these algorithms need to retain a large amount of labeled/unlabeled samples for graph learning, leading to their impracticality for real tracking applications. Codebook Learning-Based Discriminative Appearance ModelsCLDAMs need to constructed the foreground and background codebooks to adaptively capture the dynamic appearance information from the foreground and background. 实际上codebook需要的训练集太大，在实际应用中不是很方便。 Hybrid Generative-Discriminative Appearance Modelsthe generative and the discriminative mod- els have their own advantages and disadvantages and are complementary to each other to some extent.结合产生和判别模型的优点。","tags":[{"name":"Object Tracking","slug":"Object-Tracking","permalink":"http://weleen.github.io/tags/Object-Tracking/"}]}]